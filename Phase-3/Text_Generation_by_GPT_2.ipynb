{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Text_Generation_by_GPT_2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3Lq0Pzm2mbX"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqhbGVmm21wB"
      },
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "from nltk.translate.bleu_score import SmoothingFunction, corpus_bleu, sentence_bleu\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxhR17Bu21x6"
      },
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2', pad_token_id = tokenizer.eos_token_id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2llnPbsrA7R"
      },
      "source": [
        "df = pd.read_excel('Data_SOC.xlsx')\n",
        "df = pd.DataFrame(df)\n",
        "df1 = pd.DataFrame(columns={'Finance','Sc_Literature','Music/Entertainment'})\n",
        "df1.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bd_IsnmpqtWV",
        "outputId": "219a96ca-bdba-4ac9-87eb-9d231ed86ac5"
      },
      "source": [
        "i = 0\n",
        "while( i < len(df)):\n",
        "  content = df['Sc_Literature'][i]\n",
        "  input_ids = tokenizer.encode(content[:100], return_tensors='pt')\n",
        "  output = model.generate(input_ids, max_length = 1000, do_sample = True, top_k = 100, top_p = 0.9)\n",
        "\n",
        "  reference = tokenizer.decode(output[0], skip_special_tokens= True).split()\n",
        "\n",
        "  len1 = len(content)\n",
        "  len2 = len(reference)\n",
        "  if(len1 >= len2):\n",
        "    hypothesis = content.split()[:len2]\n",
        "  else:\n",
        "    reference = reference[:len1]\n",
        "    hypothesis = content.split()\n",
        "\n",
        "  smoothie = SmoothingFunction().method7\n",
        "  BLEUscore = nltk.translate.bleu_score.corpus_bleu([reference], [hypothesis], smoothing_function=smoothie)\n",
        "  df1.loc[i,'Sc_Literature'] = BLEUscore\n",
        "  print(BLEUscore)\n",
        "  i += 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.36543538581879464\n",
            "0.3654130719887559\n",
            "0.365090277220685\n",
            "0.3636005413293584\n",
            "0.3640519332520572\n",
            "0.3649279282836811\n",
            "0.3636005413293584\n",
            "0.3655038242770524\n",
            "0.3652519273037479\n",
            "0.36592350714044847\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G5MXoKbssisd",
        "outputId": "76de6afe-7a76-420c-c74e-62262a27752c"
      },
      "source": [
        "i = 0\n",
        "while( i != len(df)):\n",
        "  content = df['Finance'][i]\n",
        "  input_ids = tokenizer.encode(content[:100], return_tensors='pt')\n",
        "  output = model.generate(input_ids, max_length = 1000, do_sample = True, top_k = 100, top_p = 0.9)\n",
        "\n",
        "  reference = tokenizer.decode(output[0], skip_special_tokens= True).split()\n",
        "  \n",
        "  len1 = len(content)\n",
        "  len2 = len(reference)\n",
        "  if(len1 >= len2):\n",
        "    hypothesis = content.split()[:len2]\n",
        "  else:\n",
        "    reference = reference[:len1]\n",
        "    hypothesis = content.split()\n",
        "\n",
        "  smoothie = SmoothingFunction().method7\n",
        "  BLEUscore = nltk.translate.bleu_score.corpus_bleu([reference], [hypothesis], smoothing_function=smoothie)\n",
        "  df1.loc[i,'Finance'] = BLEUscore\n",
        "  print(BLEUscore)\n",
        "  i += 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.3642517773942746\n",
            "0.3663022407160805\n",
            "0\n",
            "0\n",
            "0.3657922229596879\n",
            "0.36601028429925375\n",
            "0.36557036943582855\n",
            "0.36592350714044847\n",
            "0.3491969342126689\n",
            "0.366874131073038\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qWl3sjcO0U1K",
        "outputId": "6b55ab84-dc83-4eeb-d441-ec923b88a8d0"
      },
      "source": [
        "i = 0\n",
        "while( i != len(df)):\n",
        "  content = df['Music/Entertainment'][i]\n",
        "  input_ids = tokenizer.encode(content[:100], return_tensors='pt')\n",
        "  output = model.generate(input_ids, max_length = 1000, do_sample = True, top_k = 100, top_p = 0.9)\n",
        "\n",
        "  reference = tokenizer.decode(output[0], skip_special_tokens= True).split()\n",
        "\n",
        "  len1 = len(content)\n",
        "  len2 = len(reference)\n",
        "  if(len1 >= len2):\n",
        "    hypothesis = content.split()[:len2]\n",
        "  else:\n",
        "    reference = reference[:len1]\n",
        "    hypothesis = content.split()\n",
        "\n",
        "  smoothie = SmoothingFunction().method7\n",
        "  BLEUscore = nltk.translate.bleu_score.corpus_bleu([reference], [hypothesis], smoothing_function=smoothie)\n",
        "  df1.loc[i,'Music/Entertainment'] = BLEUscore\n",
        "  print(BLEUscore)\n",
        "  i += 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.36459426919010207\n",
            "0.36826655206616193\n",
            "0.3582030837768331\n",
            "0.3657922229596879\n",
            "0.36574403671838307\n",
            "0.3688163311050085\n",
            "0.36716660741460677\n",
            "0.36763133738393583\n",
            "0.35706319452284657\n",
            "0.3661755750852203\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1i5Zl7rxNP9"
      },
      "source": [
        "df1.to_csv(r'BLEU_Scores_GPT-2.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYuLvWucIpkk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSuW5oB7IpmV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTlDEMLiIpoi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mchHHwI4Ipq0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-nV1TIs22DJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}